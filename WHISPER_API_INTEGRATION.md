# ğŸ¤ HÆ°á»›ng Dáº«n TÃ­ch Há»£p Whisper API cho Speech-to-Text

## ğŸ“‹ Tá»•ng Quan

Whisper API cá»§a OpenAI cung cáº¥p Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n Web Speech API, Ä‘áº·c biá»‡t vá»›i tiáº¿ng Viá»‡t. CÃ³ 2 cÃ¡ch tÃ­ch há»£p:

1. **Whisper API (Cloud)** - Dá»… setup, cáº§n API key, cÃ³ phÃ­
2. **Whisper Local** - Miá»…n phÃ­, cáº§n backend server

---

## ğŸ¯ Option 1: Whisper API (OpenAI Cloud) - Khuyáº¿n Nghá»‹

### **Æ¯u Äiá»ƒm:**
- âœ… Dá»… tÃ­ch há»£p (chá»‰ cáº§n API key)
- âœ… KhÃ´ng cáº§n backend server
- âœ… Äá»™ chÃ­nh xÃ¡c cao
- âœ… Há»— trá»£ nhiá»u Ä‘á»‹nh dáº¡ng audio

### **NhÆ°á»£c Äiá»ƒm:**
- âš ï¸ Cáº§n internet
- âš ï¸ CÃ³ phÃ­: $0.006/phÃºt (~$0.36/giá»)
- âš ï¸ Data gá»­i lÃªn OpenAI server

### **Setup Steps:**

#### **1. CÃ i Äáº·t Dependencies:**
```bash
npm install openai
```

#### **2. Táº¡o API Key:**
- ÄÄƒng kÃ½ táº¡i: https://platform.openai.com/
- Táº¡o API key táº¡i: https://platform.openai.com/api-keys
- ThÃªm vÃ o `.env`:
```env
VITE_OPENAI_API_KEY=sk-...
```

#### **3. Táº¡o Hook má»›i: `useWhisperRecognition.js`**

File nÃ y sáº½ thay tháº¿ `useSpeechRecognition.js` hoáº·c dÃ¹ng song song.

---

## ğŸ¯ Option 2: Whisper Local (Backend Server)

### **Æ¯u Äiá»ƒm:**
- âœ… Miá»…n phÃ­ (khÃ´ng cÃ³ phÃ­ API)
- âœ… Privacy (data khÃ´ng gá»­i lÃªn cloud)
- âœ… KhÃ´ng cáº§n internet (sau khi setup)

### **NhÆ°á»£c Äiá»ƒm:**
- âš ï¸ Cáº§n backend server (Python + Whisper model)
- âš ï¸ Setup phá»©c táº¡p hÆ¡n
- âš ï¸ Cáº§n GPU Ä‘á»ƒ cháº¡y nhanh (CPU cÅ©ng Ä‘Æ°á»£c nhÆ°ng cháº­m)

### **Setup Steps:**

#### **1. Táº¡o Backend Server (Python):**
```bash
mkdir whisper-backend
cd whisper-backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install fastapi uvicorn openai-whisper python-multipart
```

#### **2. Táº¡o API Endpoint:**
File `app.py`:
```python
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
import whisper
import io
import tempfile
import os

app = FastAPI()

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Hoáº·c chá»‰ frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load Whisper model (chá»‰ load 1 láº§n khi start server)
model = whisper.load_model("base")  # base, small, medium, large

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    # Save audio to temp file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name
    
    try:
        # Transcribe
        result = model.transcribe(tmp_path, language="vi")
        return {"text": result["text"]}
    finally:
        # Cleanup
        os.unlink(tmp_path)
```

#### **3. Cháº¡y Server:**
```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

#### **4. Expose qua Cloudflare Tunnel (náº¿u cáº§n):**
```bash
cloudflared tunnel --url http://localhost:8000
```

---

## ğŸ’» Implementation Code

Xem cÃ¡c file sau:
- `src/hooks/useWhisperRecognition.js` - Hook cho Whisper API (Cloud)
- `src/hooks/useWhisperLocal.js` - Hook cho Whisper Local (Backend)
- `src/components/VoiceOrderButton.jsx` - Updated Ä‘á»ƒ há»— trá»£ cáº£ 2 options

